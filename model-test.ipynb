{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In dit notebook worden de LLIE-methoden getest om data te genereren. Hierna worden er voorspellingen gemaakt met het valdetectiemodel om metrics en inzichten te verzamelen.\n",
    "\n",
    "#### Onderstaande code laadt het valdetectiemodel in, zodat hier later mee gewerkt kan worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow.lite as tflite\n",
    "\n",
    "\n",
    "MODEL_FILENAME = 'model.tflite'\n",
    "LABELS_FILENAME = 'labels.txt'\n",
    "\n",
    "od_model = None\n",
    "labels = None\n",
    "\n",
    "\n",
    "class ObjectDetection:\n",
    "    OUTPUT_TENSOR_NAMES = ['detected_boxes', 'detected_scores', 'detected_classes']\n",
    "\n",
    "    def __init__(self, model_filename):\n",
    "        self._interpreter = tflite.Interpreter(model_path=model_filename)\n",
    "        self._interpreter.allocate_tensors()\n",
    "\n",
    "        input_details = self._interpreter.get_input_details()\n",
    "        output_details = self._interpreter.get_output_details()\n",
    "        assert len(input_details) == 1\n",
    "        self._input_index = input_details[0]['index']\n",
    "\n",
    "        # Get dictionary with output details {name: index}\n",
    "        output_name_index = {d['name']: d['index'] for d in output_details}\n",
    "        self._output_indexes = [output_name_index[name] for name in self.OUTPUT_TENSOR_NAMES]\n",
    "\n",
    "        self._input_size = int(input_details[0]['shape'][1])\n",
    "\n",
    "    def predict_image(self, image):\n",
    "        image = image.convert('RGB') if image.mode != 'RGB' else image\n",
    "        image = image.resize((self._input_size, self._input_size))\n",
    "\n",
    "        input_array = np.array(image, dtype=np.float32)[np.newaxis, :, :, :]\n",
    "        self._interpreter.set_tensor(self._input_index, input_array)\n",
    "        self._interpreter.invoke()\n",
    "\n",
    "        outputs = [self._interpreter.get_tensor(i) for i in self._output_indexes]\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def initialize():\n",
    "    global od_model\n",
    "    od_model = ObjectDetection(MODEL_FILENAME)\n",
    "    global labels\n",
    "    with open(LABELS_FILENAME) as f:\n",
    "        labels = [label.strip() for label in f.readlines()]\n",
    "\n",
    "\n",
    "def predict_url(image_url):\n",
    "    with urlopen(image_url) as binary:\n",
    "        image = PIL.Image.open(binary)\n",
    "        return predict_image(image)\n",
    "\n",
    "\n",
    "def predict_image(image):\n",
    "    assert od_model is not None\n",
    "    assert labels is not None\n",
    "\n",
    "    predictions = od_model.predict_image(image)\n",
    "\n",
    "    predictions = [{'probability': round(float(p[1]), 8),\n",
    "                    'tagId': int(p[2]),\n",
    "                    'tagName': labels[p[2]],\n",
    "                    'boundingBox': {\n",
    "                        'left': round(float(p[0][0]), 8),\n",
    "                        'top': round(float(p[0][1]), 8),\n",
    "                        'width': round(float(p[0][2] - p[0][0]), 8),\n",
    "                        'height': round(float(p[0][3] - p[0][1]), 8)\n",
    "                        }\n",
    "                    } for p in zip(*predictions)]\n",
    "\n",
    "    #best_prediction = max(predictions, key=lambda p: p['probability'])\n",
    "    response = {'id': '', 'project': '', 'iteration': '', 'created': datetime.utcnow().isoformat(),\n",
    "                'predictions': predictions}\n",
    "\n",
    "    # print(\"Results: \" + str(response))\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Onderstaande code geeft de logica voor het voorspellen, berekenen van de metrics, en het tekenen van de bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize()\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_image_matplotlib(image, title=\"\"):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def draw_bounding_box_with_title(image, bbox, predicted_label, probability, title=\"\"):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Afmetingen in pixels berekenen\n",
    "    width, height = image.size\n",
    "    left = bbox['left'] * width\n",
    "    top = bbox['top'] * height\n",
    "    box_width = bbox['width'] * width\n",
    "    box_height = bbox['height'] * height\n",
    "\n",
    "    # Bounding box tekenen\n",
    "    rect = patches.Rectangle((left, top), box_width, box_height,\n",
    "                             linewidth=2, edgecolor='red', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    # Tekst met label + probability op bounding box zetten\n",
    "    label_text = f\"{predicted_label} ({probability:.2f})\"\n",
    "    ax.text(left, top - 10, label_text, color='white', fontsize=12,\n",
    "            backgroundcolor='red')\n",
    "\n",
    "    # Titel van het figuur instellen\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def predict_falls(base_path, THRESHOLD):\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    total_predictions = 0\n",
    "    \n",
    "    TP = 0  # true positives\n",
    "    FP = 0  # false positives\n",
    "    TN = 0  # true negatives\n",
    "    FN = 0  # false negatives\n",
    "\n",
    "    POSE_LABELS = [\"laying\", \"sitting\", \"standing\", \"bending\"]\n",
    "\n",
    "    def is_laying_true(filename):\n",
    "        if \"_not_laying\" in filename:\n",
    "            return False\n",
    "        elif \"_laying\" in filename:\n",
    "            return True\n",
    "\n",
    "    def is_laying_pred(prediction):\n",
    "        return (\n",
    "            prediction['tagName'] == \"laying\"\n",
    "            and prediction['probability'] >= THRESHOLD\n",
    "        )\n",
    "\n",
    "    for i in range(1, 32):\n",
    "        room_path = os.path.join(base_path, f\"room{i}\")\n",
    "        if not os.path.isdir(room_path):\n",
    "            print(f\"{room_path} bestaat niet, overslaan...\")\n",
    "            continue\n",
    "\n",
    "        for filename in os.listdir(room_path):\n",
    "            if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                continue  # skip geen afbeeldingen\n",
    "\n",
    "            filepath = os.path.join(room_path, filename)\n",
    "            image = PIL.Image.open(filepath)\n",
    "\n",
    "            try:\n",
    "                result = predict_image(image)\n",
    "                total_predictions += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Fout bij voorspellen van {filepath}: {e}\")\n",
    "                continue\n",
    "\n",
    "            predictions = result['predictions']\n",
    "            if not predictions:\n",
    "                print(f\"Geen voorspelling voor {filename}\")\n",
    "                continue\n",
    "\n",
    "            pose_predictions = [p for p in predictions if p['tagName'] in POSE_LABELS]\n",
    "\n",
    "            if pose_predictions:\n",
    "                best_pred = max(pose_predictions, key=lambda p: p['probability'])\n",
    "                predicted = is_laying_pred(best_pred)\n",
    "            else:\n",
    "                predicted = False\n",
    "                \n",
    "            actual = is_laying_true(filename)\n",
    "\n",
    "            if actual and predicted:\n",
    "                TP += 1\n",
    "            elif not actual and predicted:\n",
    "                FP += 1\n",
    "                print(f\"[FALSE POSITIVE] Room: room{i}, File: {filename}\")\n",
    "                draw_bounding_box_with_title(\n",
    "                    image,\n",
    "                    bbox=best_pred['boundingBox'],\n",
    "                    predicted_label=best_pred['tagName'],\n",
    "                    probability=best_pred['probability'],\n",
    "                    title=f\"False Positive - room{i}/{filename}\"\n",
    "                )\n",
    "            elif not actual and not predicted:\n",
    "                TN += 1\n",
    "            elif actual and not predicted:\n",
    "                FN += 1\n",
    "                print(f\"[FALSE NEGATIVE] Room: room{i}, File: {filename}\")\n",
    "                draw_bounding_box_with_title(\n",
    "                    image,\n",
    "                    bbox=best_pred['boundingBox'],\n",
    "                    predicted_label=best_pred['tagName'],\n",
    "                    probability=best_pred['probability'],\n",
    "                    title=f\"False Positive - room{i}/{filename}\"\n",
    "                )\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    avg_time_per_prediction = total_time / total_predictions if total_predictions > 0 else 0\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "    print(\"\\n=== Resultaten ===\")\n",
    "    print(f\"TP: {TP}, FP: {FP}, TN: {TN}, FN: {FN}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"\\n=== Timing ===\")\n",
    "    print(f\"Totale tijd: {total_time:.2f} seconden\")\n",
    "    print(f\"Aantal voorspellingen: {total_predictions}\")\n",
    "    print(f\"Gemiddelde tijd per voorspelling: {avg_time_per_prediction:.4f} seconden\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline waarden, om later op te kunnen vergelijken. elke predict_falls() is steeds 2 keer uitgevoerd, met threshold 0.05 en 0.5. Omdat het ingeladen van alle afbeeldingen in de notebook veel opslag en overzicht kost, zijn niet alle resultaten bewaard gebleven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_falls(\"data-filtered-rooms\", 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traditionele methoden setup, beginnend met LIME https://github.com/aeinrw/LIME/blob/master/LIME_CLI.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import fft\n",
    "from skimage import io, exposure, img_as_ubyte, img_as_float\n",
    "from tqdm import trange\n",
    "from PIL import Image\n",
    "\n",
    "# ------------------ LIME IMPLEMENTATIE ------------------ #\n",
    "def firstOrderDerivative(n, k=1):\n",
    "    return np.eye(n) * (-1) + np.eye(n, k=k)\n",
    "\n",
    "def toeplitizMatrix(n, row):\n",
    "    vecDD = np.zeros(n)\n",
    "    vecDD[0] = 4\n",
    "    vecDD[1] = -1\n",
    "    vecDD[row] = -1\n",
    "    vecDD[-1] = -1\n",
    "    vecDD[-row] = -1\n",
    "    return vecDD\n",
    "\n",
    "def vectorize(matrix):\n",
    "    return matrix.T.ravel()\n",
    "\n",
    "def reshape(vector, row, col):\n",
    "    return vector.reshape((row, col), order='F')\n",
    "\n",
    "class LIME:\n",
    "    def __init__(self, iterations=10, alpha=2, rho=2, gamma=0.7, strategy=2):\n",
    "        self.iterations = iterations\n",
    "        self.alpha = alpha\n",
    "        self.rho = rho\n",
    "        self.gamma = gamma\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def load(self, imgPath):\n",
    "        self.L = img_as_float(io.imread(imgPath))\n",
    "        self.row, self.col = self.L.shape[:2]\n",
    "        self.T_hat = np.max(self.L, axis=2)\n",
    "        self.dv = firstOrderDerivative(self.row)\n",
    "        self.dh = firstOrderDerivative(self.col, -1)\n",
    "        self.vecDD = toeplitizMatrix(self.row * self.col, self.row)\n",
    "        self.W = self.weightingStrategy()\n",
    "\n",
    "    def weightingStrategy(self):\n",
    "        if self.strategy == 2:\n",
    "            dTv = self.dv @ self.T_hat\n",
    "            dTh = self.T_hat @ self.dh\n",
    "            Wv = 1 / (np.abs(dTv) + 1)\n",
    "            Wh = 1 / (np.abs(dTh) + 1)\n",
    "            return np.vstack([Wv, Wh])\n",
    "        else:\n",
    "            return np.ones((self.row * 2, self.col))\n",
    "\n",
    "    def __T_subproblem(self, G, Z, u):\n",
    "        X = G - Z / u\n",
    "        Xv = X[:self.row, :]\n",
    "        Xh = X[self.row:, :]\n",
    "        temp = self.dv @ Xv + Xh @ self.dh\n",
    "        numerator = fft.fft(vectorize(2 * self.T_hat + u * temp))\n",
    "        denominator = fft.fft(self.vecDD * u) + 2\n",
    "        T = fft.ifft(numerator / denominator)\n",
    "        return exposure.rescale_intensity(np.real(reshape(T, self.row, self.col)), (0, 1), (0.001, 1))\n",
    "\n",
    "    def __G_subproblem(self, T, Z, u, W):\n",
    "        dT = self.__derivative(T)\n",
    "        epsilon = self.alpha * W / u\n",
    "        X = dT + Z / u\n",
    "        return np.sign(X) * np.maximum(np.abs(X) - epsilon, 0)\n",
    "\n",
    "    def __Z_subproblem(self, T, G, Z, u):\n",
    "        dT = self.__derivative(T)\n",
    "        return Z + u * (dT - G)\n",
    "\n",
    "    def __u_subproblem(self, u):\n",
    "        return u * self.rho\n",
    "\n",
    "    def __derivative(self, matrix):\n",
    "        v = self.dv @ matrix\n",
    "        h = matrix @ self.dh\n",
    "        return np.vstack([v, h])\n",
    "\n",
    "    def illumMap(self):\n",
    "        T = np.zeros((self.row, self.col))\n",
    "        G = np.zeros((self.row * 2, self.col))\n",
    "        Z = np.zeros((self.row * 2, self.col))\n",
    "        u = 1\n",
    "\n",
    "        for _ in trange(0, self.iterations, leave=False):\n",
    "            T = self.__T_subproblem(G, Z, u)\n",
    "            G = self.__G_subproblem(T, Z, u, self.W)\n",
    "            Z = self.__Z_subproblem(T, G, Z, u)\n",
    "            u = self.__u_subproblem(u)\n",
    "\n",
    "        return T ** self.gamma\n",
    "\n",
    "    def enhance(self):\n",
    "        self.T = self.illumMap()\n",
    "        self.R = self.L / np.repeat(self.T[:, :, np.newaxis], 3, axis=2)\n",
    "        self.R = exposure.rescale_intensity(self.R, (0, 1))\n",
    "        self.R = img_as_ubyte(self.R)\n",
    "        return self.R\n",
    "\n",
    "# ------------------ BATCH VERWERKING ------------------ #\n",
    "\n",
    "def process_folder(input_root, output_root):\n",
    "    lime = LIME()\n",
    "\n",
    "    for root, dirs, files in os.walk(input_root):\n",
    "        for file in files:\n",
    "            if not file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                continue\n",
    "\n",
    "            input_path = os.path.join(root, file)\n",
    "            rel_path = os.path.relpath(input_path, input_root)\n",
    "            output_path = os.path.join(output_root, rel_path)\n",
    "\n",
    "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "            try:\n",
    "                lime.load(input_path)\n",
    "                enhanced = lime.enhance()\n",
    "                Image.fromarray(enhanced).save(output_path)\n",
    "                print(f\"{rel_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Fout bij {rel_path}: {e}\")\n",
    "\n",
    "\n",
    "input_root = \"data-filtered-rooms\"           # map met room1, room2, ...\n",
    "output_root = \"data-lime2\"      # waar het resultaat komt\n",
    "\n",
    "process_folder(input_root, output_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Een aantal wat simpelere methoden, gebaseerd op OpenCV-documentatie\n",
    "\n",
    "https://docs.opencv.org/3.4/d3/dc1/tutorial_basic_linear_transform.html <br>\n",
    "https://docs.opencv.org/4.x/d5/daf/tutorial_py_histogram_equalization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import img_as_float\n",
    "from PIL import Image\n",
    "\n",
    "def apply_clahe(img):\n",
    "    img_yuv = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    img_yuv[:, :, 0] = clahe.apply(img_yuv[:, :, 0])\n",
    "    return cv2.cvtColor(img_yuv, cv2.COLOR_YUV2RGB)\n",
    "\n",
    "def gamma_correction(img, gamma=1.5):\n",
    "    inv_gamma = 1.0 / gamma\n",
    "    table = np.array([(i / 255.0) ** inv_gamma * 255 for i in range(256)]).astype(\"uint8\")\n",
    "    return cv2.LUT(img, table)\n",
    "\n",
    "def lime_enhance(img):\n",
    "    illum_map = np.max(img / 255.0, axis=2)\n",
    "    illum_map = cv2.GaussianBlur(illum_map, (5, 5), 0)\n",
    "    enhanced = np.clip(img / (illum_map[..., None] + 1e-3), 0, 255).astype(np.uint8)\n",
    "    return enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"data-filtered-rooms\"\n",
    "output_base = \"data-{method}\"  # Placeholder\n",
    "\n",
    "methods = {\n",
    "    \"clahe\": apply_clahe,\n",
    "    \"gamma\": lambda img: gamma_correction(img, gamma=1.5),\n",
    "    \"lime\": lime_enhance\n",
    "}\n",
    "\n",
    "for method_name, func in methods.items():\n",
    "    print(f\"Methode: {method_name}\")\n",
    "\n",
    "    for i in range(1, 32):\n",
    "        room_path = os.path.join(base_path, f\"room{i}\")\n",
    "        if not os.path.isdir(room_path):\n",
    "            print(f\"{room_path} bestaat niet, overslaan...\")\n",
    "            continue\n",
    "\n",
    "        output_path = os.path.join(output_base.format(method=method_name), f\"room{i}\")\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        for filename in os.listdir(room_path):\n",
    "            if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                continue\n",
    "\n",
    "            input_file = os.path.join(room_path, filename)\n",
    "            output_file = os.path.join(output_path, filename)\n",
    "\n",
    "            try:\n",
    "                # Open als numpy-array\n",
    "                with Image.open(input_file) as img_pil:\n",
    "                    img = np.array(img_pil.convert(\"RGB\"))\n",
    "\n",
    "                # Pas LLIE methode toe\n",
    "                enhanced = func(img)\n",
    "\n",
    "                # Opslaan\n",
    "                Image.fromarray(enhanced).save(output_file)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Fout bij {input_file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSRCR code van https://github.com/muggledy/retinex/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n",
      "gelukt\n"
     ]
    }
   ],
   "source": [
    "def MultiScaleRetinex(img,sigmas=[15,80,250],weights=None,flag=True):\n",
    "    '''equal to func retinex_MSR, just remove the outer for-loop. Practice has proven \n",
    "       that when MSR used in MSRCR or Gimp, we should add stretch step, otherwise the \n",
    "       result color may be dim. But it's up to you, if you select to neglect stretch, \n",
    "       set flag as False, have fun'''\n",
    "    if weights==None:\n",
    "        weights=np.ones(len(sigmas))/len(sigmas)\n",
    "    elif not abs(sum(weights)-1)<0.00001:\n",
    "        raise ValueError('sum of weights must be 1!')\n",
    "    r=np.zeros(img.shape,dtype='double')\n",
    "    img=img.astype('double')\n",
    "    for i,sigma in enumerate(sigmas):\n",
    "        r+=(np.log(img+1)-np.log(gauss_blur(img,sigma)+1))*weights[i]\n",
    "    if flag:\n",
    "        mmin=np.min(r,axis=(0,1),keepdims=True)\n",
    "        mmax=np.max(r,axis=(0,1),keepdims=True)\n",
    "        r=(r-mmin)/(mmax-mmin)*255 #maybe indispensable when used in MSRCR or Gimp, make pic vibrant\n",
    "        r=r.astype('uint8')\n",
    "    return r\n",
    "\n",
    "\n",
    "def simplest_color_balance(img_msrcr,s1,s2):\n",
    "    '''see section 3.1 in “Simplest Color Balance”(doi: 10.5201/ipol.2011.llmps-scb). \n",
    "    Only suitable for 1-channel image'''\n",
    "    sort_img=np.sort(img_msrcr,None)\n",
    "    N=img_msrcr.size\n",
    "    Vmin=sort_img[int(N*s1)]\n",
    "    Vmax=sort_img[int(N*(1-s2))-1]\n",
    "    img_msrcr[img_msrcr<Vmin]=Vmin\n",
    "    img_msrcr[img_msrcr>Vmax]=Vmax\n",
    "    return (img_msrcr-Vmin)*255/(Vmax-Vmin)\n",
    "\n",
    "def retinex_MSRCR(img,sigmas=[12,80,250],s1=0.01,s2=0.01):\n",
    "    '''r=βlog(αI')MSR, I'=I/∑I, I is one channel of image, ∑I is the sum of all channels, \n",
    "       C:=βlog(αI') is named as color recovery factor. Last we improve previously used \n",
    "       linear stretch: MSRCR:=r, r=G[MSRCR-b], then doing linear stretch. In practice, it \n",
    "       doesn't work well, so we take another measure: Simplest Color Balance'''\n",
    "    alpha=125\n",
    "    img=img.astype('double')+1 #\n",
    "    csum_log=np.log(np.sum(img,axis=2))\n",
    "    msr=MultiScaleRetinex(img-1,sigmas) #-1\n",
    "    r=(np.log(alpha*img)-csum_log[...,None])*msr\n",
    "    #beta=46;G=192;b=-30;r=G*(beta*r-b) #deprecated\n",
    "    #mmin,mmax=np.min(r),np.max(r)\n",
    "    #stretch=(r-mmin)/(mmax-mmin)*255 #linear stretch is unsatisfactory\n",
    "    for i in range(r.shape[-1]):\n",
    "        r[...,i]=simplest_color_balance(r[...,i],0.01,0.01)\n",
    "    return r.astype('uint8')\n",
    "\n",
    "\n",
    "def get_gauss_kernel(sigma,dim=2):\n",
    "    '''1D gaussian function: G(x)=1/(sqrt{2π}σ)exp{-(x-μ)²/2σ²}. Herein, μ:=0, after \n",
    "       normalizing the 1D kernel, we can get 2D kernel version by \n",
    "       matmul(1D_kernel',1D_kernel), having same sigma in both directions. Note that \n",
    "       if you want to blur one image with a 2-D gaussian filter, you should separate \n",
    "       it into two steps(i.e. separate the 2-D filter into two 1-D filter, one column \n",
    "       filter, one row filter): 1) blur image with first column filter, 2) blur the \n",
    "       result image of 1) with the second row filter. Analyse the time complexity: if \n",
    "       m&n is the shape of image, p&q is the size of 2-D filter, bluring image with \n",
    "       2-D filter takes O(mnpq), but two-step method takes O(pmn+qmn)'''\n",
    "    ksize=int(np.floor(sigma*6)/2)*2+1 #kernel size(\"3-σ\"法则) refer to \n",
    "    #https://github.com/upcAutoLang/MSRCR-Restoration/blob/master/src/MSRCR.cpp\n",
    "    k_1D=np.arange(ksize)-ksize//2\n",
    "    k_1D=np.exp(-k_1D**2/(2*sigma**2))\n",
    "    k_1D=k_1D/np.sum(k_1D)\n",
    "    if dim==1:\n",
    "        return k_1D\n",
    "    elif dim==2:\n",
    "        return k_1D[:,None].dot(k_1D.reshape(1,-1))\n",
    "\n",
    "def gauss_blur_original(img,sigma):\n",
    "    '''suitable for 1 or 3 channel image'''\n",
    "    row_filter=get_gauss_kernel(sigma,1)\n",
    "    t=cv2.filter2D(img,-1,row_filter[...,None])\n",
    "    return cv2.filter2D(t,-1,row_filter.reshape(1,-1))\n",
    "\n",
    "def gauss_blur_recursive(img,sigma):\n",
    "    '''refer to “Recursive implementation of the Gaussian filter”\n",
    "       (doi: 10.1016/0165-1684(95)00020-E). Paper considers it faster than \n",
    "       FFT(Fast Fourier Transform) implementation of a Gaussian filter. \n",
    "       Suitable for 1 or 3 channel image'''\n",
    "    pass\n",
    "\n",
    "def gauss_blur(img,sigma,method='original'):\n",
    "    if method=='original':\n",
    "        return gauss_blur_original(img,sigma)\n",
    "    elif method=='recursive':\n",
    "        return gauss_blur_recursive(img,sigma)\n",
    "    \n",
    "\n",
    "base_path = \"data-filtered-rooms\"\n",
    "output_base = \"data-msrcr\"\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "for i in range(1, 32):\n",
    "    room_path = os.path.join(base_path, f\"room{i}\")\n",
    "    if not os.path.isdir(room_path):\n",
    "        print(f\"{room_path} bestaat niet, overslaan...\")\n",
    "        continue\n",
    "\n",
    "    output_path = os.path.join(output_base.format(method=\"MSRCR\"), f\"room{i}\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(room_path):\n",
    "        if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            continue\n",
    "\n",
    "        input_file = os.path.join(room_path, filename)\n",
    "        output_file = os.path.join(output_path, filename)\n",
    "\n",
    "        try:\n",
    "            # Lees afbeelding in met OpenCV en converteer naar RGB\n",
    "            img_bgr = cv2.imread(input_file)\n",
    "            if img_bgr is None:\n",
    "                raise ValueError(\"Afbeelding kon niet worden ingelezen\")\n",
    "            img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Pas LLIE-methode toe\n",
    "            enhanced_rgb = retinex_MSRCR(img_rgb)\n",
    "\n",
    "            # Converteer terug naar BGR voor correcte opslag met OpenCV\n",
    "            enhanced_bgr = cv2.cvtColor(enhanced_rgb, cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(output_file, enhanced_bgr)\n",
    "            print(\"gelukt\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Fout bij {input_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_falls(\"data-clahe\", 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_falls(\"data-gamma\", 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_falls(\"data-lime\", 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_falls(\"data-lime2\", 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_falls(\"data-msrcr\", 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep learning-resultaten, de afbeeldingen zijn gegenereerd in ZeroDceLite/finetune.ipynb (door het niet gefinetunede model in te laten en te voorspellen) en RetinexNet/test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_falls(\"data-retinexnet\", 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_falls(\"data-zerodce\", 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_falls(\"data-zerodce-lite\", 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NIQE-metric berekenen via BasicSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verwerken van datasetmap: data-archief\n",
      "Geen geldige afbeeldingen gevonden in 'data-archief'\n",
      "\n",
      "Verwerken van datasetmap: data-clahe\n",
      "Gemiddelde NIQE-score voor 'data-clahe': 4.3612\n",
      "\n",
      "Verwerken van datasetmap: data-filtered-rooms\n",
      "Gemiddelde NIQE-score voor 'data-filtered-rooms': 5.3462\n",
      "\n",
      "Verwerken van datasetmap: data-gamma\n",
      "Gemiddelde NIQE-score voor 'data-gamma': 4.6719\n",
      "\n",
      "Verwerken van datasetmap: data-lime\n",
      "Gemiddelde NIQE-score voor 'data-lime': 4.2360\n",
      "\n",
      "Verwerken van datasetmap: data-lime2\n",
      "Gemiddelde NIQE-score voor 'data-lime2': 4.2768\n",
      "\n",
      "Verwerken van datasetmap: data-mirnet\n",
      "Gemiddelde NIQE-score voor 'data-mirnet': 4.0885\n",
      "\n",
      "Verwerken van datasetmap: data-msrcr\n",
      "Gemiddelde NIQE-score voor 'data-msrcr': 4.1169\n",
      "\n",
      "Verwerken van datasetmap: data-retinex\n",
      "Gemiddelde NIQE-score voor 'data-retinex': 5.3965\n",
      "\n",
      "Verwerken van datasetmap: data-retinexnet\n",
      "Gemiddelde NIQE-score voor 'data-retinexnet': 4.0092\n",
      "\n",
      "Verwerken van datasetmap: data-uretinex\n",
      "Geen geldige afbeeldingen gevonden in 'data-uretinex'\n",
      "\n",
      "Verwerken van datasetmap: data-zero-dce-finetuned\n",
      "Gemiddelde NIQE-score voor 'data-zero-dce-finetuned': 4.9520\n",
      "\n",
      "Verwerken van datasetmap: data-zero-dce-finetuned100\n",
      "Gemiddelde NIQE-score voor 'data-zero-dce-finetuned100': 5.0237\n",
      "\n",
      "Verwerken van datasetmap: data-zero-dce-finetuned1006\n",
      "Gemiddelde NIQE-score voor 'data-zero-dce-finetuned1006': 5.0267\n",
      "\n",
      "Verwerken van datasetmap: data-zero-dce-finetuned50\n",
      "Gemiddelde NIQE-score voor 'data-zero-dce-finetuned50': 5.0274\n",
      "\n",
      "Verwerken van datasetmap: data-zerodcc\n",
      "Gemiddelde NIQE-score voor 'data-zerodcc': 4.5827\n",
      "\n",
      "Verwerken van datasetmap: data-zerodcc-tf\n",
      "Gemiddelde NIQE-score voor 'data-zerodcc-tf': 3.8249\n",
      "\n",
      "Verwerken van datasetmap: data-zerodce\n",
      "Gemiddelde NIQE-score voor 'data-zerodce': 3.6519\n",
      "\n",
      "Verwerken van datasetmap: data-zerodce-lite\n",
      "Gemiddelde NIQE-score voor 'data-zerodce-lite': 3.8262\n",
      "\n",
      "Verwerken van datasetmap: datasets\n",
      "Geen geldige afbeeldingen gevonden in 'datasets'\n",
      "\n",
      "Verwerken van datasetmap: test_dataset\n",
      "Geen geldige afbeeldingen gevonden in 'test_dataset'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from basicsr.metrics.niqe import calculate_niqe\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))\n",
    "\n",
    "def calculate_avg_niqe_in_data_folder(data_folder):\n",
    "    niqe_scores = []\n",
    "\n",
    "    # Doorloop submappen die beginnen met \"room\"\n",
    "    for room_name in os.listdir(data_folder):\n",
    "        room_path = os.path.join(data_folder, room_name)\n",
    "        if os.path.isdir(room_path) and room_name.lower().startswith(\"room\"):\n",
    "            for filename in os.listdir(room_path):\n",
    "                if is_image_file(filename):\n",
    "                    img_path = os.path.join(room_path, filename)\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is None:\n",
    "                        continue\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                    h, w = img.shape\n",
    "                    if h < 128 or w < 128:\n",
    "                        img = cv2.resize(img, (128, 128), interpolation=cv2.INTER_AREA)\n",
    "                    try:\n",
    "                        score = calculate_niqe(img, crop_border=0)\n",
    "                        niqe_scores.append(score)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Fout bij berekenen NIQE voor {img_path}: {e}\")\n",
    "\n",
    "    if niqe_scores:\n",
    "        return np.mean(niqe_scores)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "base_dir=\".\"\n",
    "for entry in os.listdir(base_dir):\n",
    "    if \"data\" in entry.lower():\n",
    "        data_folder_path = os.path.join(base_dir, entry)\n",
    "        if os.path.isdir(data_folder_path):\n",
    "            print(f\"\\nVerwerken van datasetmap: {entry}\")\n",
    "            avg_niqe = calculate_avg_niqe_in_data_folder(data_folder_path)\n",
    "            if avg_niqe is not None:\n",
    "                print(f\"Gemiddelde NIQE-score voor '{entry}': {avg_niqe:.4f}\")\n",
    "            else:\n",
    "                print(f\"Geen geldige afbeeldingen gevonden in '{entry}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-onnx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
